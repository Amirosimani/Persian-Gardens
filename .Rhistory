library(twitteR)
library(dplyr)
library(twitteR)
library(lubridate)
library(RJSONIO)
library(ggplot2)
library(dismo)
install.packages("dismo")
library(dismo)
library(sp)
library(raster)
library(dismo)
library(maps)
install.packages("maps")
library(maps)
searchTerms<-c("chemical weapons", "cw", "opcw")
names(searchTerms)<-searchTerms
searchTerms<-c("chemical weapons", "cw", "opcw")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=1000)
})
})
searchTerms<-c("flu", "cold", "nausea", "vomiting", "headache")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=1000)
})
searchTwitter("#beer", n=100)
consumerKey = "TnhFkCkWw5XiRjaaU6MVFJXrb"   # from your app name
consumerSecret = "8T2gfZ7hpCRAYcBQbkfURnyT3ylaHEa8BsO2akLz6gll6kN2OM"
accessToken = "377053028-LZYBzp2rcwn3sG103AVVdUvWHYrOBHLhNPP2wq5S"
accessSecret = "eFD67fq59GaltQj45I5F5eT8wEEdlCxEG3bRE3qod8ZVo"
setup_twitter_oauth(consumer_key = consumerKey, consumer_secret = consumerSecret,
access_token = accessToken, access_secret = accessSecret)
searchTwitter("#beer", n=100)
searchTerms<-c("flu", "cold", "nausea", "vomiting", "headache")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=1000)
})
flu
print flu
print "flu
searchTwitter("#beer", n=100)
library(sp)
library(raster)
library(twitteR)
library(lubridate)
library(RJSONIO)
library(ggplot2)
library(dismo)
library(maps)
consumerKey = "TnhFkCkWw5XiRjaaU6MVFJXrb"   # from your app name
consumerSecret = "8T2gfZ7hpCRAYcBQbkfURnyT3ylaHEa8BsO2akLz6gll6kN2OM"
accessToken = "377053028-LZYBzp2rcwn3sG103AVVdUvWHYrOBHLhNPP2wq5S"
accessSecret = "eFD67fq59GaltQj45I5F5eT8wEEdlCxEG3bRE3qod8ZVo"
setup_twitter_oauth(consumer_key = consumerKey, consumer_secret = consumerSecret,
access_token = accessToken, access_secret = accessSecret)
searchTwitter("#beer", n=100)
searchTerms<-c("flu")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=10)
})
rate.limit <- getCurRateLimitInfo()
View(rate.limit)
rate.limit[rate.limit$limit != rate.limit$remaining,]
tweetFrames<-lapply(searchResults, twListToDF)
searchTerms<-c("flu")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=10)
})
tweetFrames<-lapply(searchResults, twListToDF)
tweetFrames <- lapply(tweetFrames, function(df){
df$timeStamp <- ymd_hms(as.character(df$created))
return(df)
})
nTweets <- unlist(lapply(tweetFrames, function(df){
nrow(df)
}))
timeElapsed <- unlist(lapply(tweetFrames, function(df){
as.numeric(diff(range(df$timeStamp)), units = "secs")
}))
tweetsPerSec <- nTweets
plot(tweetsPerSec, type="h", frame.plot=FALSE,  xaxt="n")
axis(1, labels=names(tweetsPerSec), at=c(1:5))
flu<-(unclass(tweetFrames$flu$timeStamp)-1356100000)
searchTerms<-c("Chemical Weapon", "chemical weapons", "OPCW", "opcw")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=10)
})
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=1000)
})
searchTerms<-c("Chemical Weapon", "chemical weapons", "OPCW")
names(searchTerms)<-searchTerms
searchResults<-lapply(searchTerms, function(tt){
print(tt)
searchTwitter(searchString=tt, n=1000)
})
tweetFrames<-lapply(searchResults, twListToDF)
tweetFrames <- lapply(tweetFrames, function(df){
df$timeStamp <- ymd_hms(as.character(df$created))
return(df)
})
nTweets <- unlist(lapply(tweetFrames, function(df){
nrow(df)
}))
timeElapsed <- unlist(lapply(tweetFrames, function(df){
as.numeric(diff(range(df$timeStamp)), units = "days")
}))
tweetsPerSec <- nTweets
plot(tweetsPerSec, type="h", frame.plot=FALSE,  xaxt="n")
axis(1, labels=names(tweetsPerSec), at=c(1:5))
flu<-(unclass(tweetFrames$flu$timeStamp)-1356100000)
tweetFrames<-lapply(searchResults, twListToDF)
tweetFrames <- lapply(tweetFrames, function(df){
df$timeStamp <- ymd_hms(as.character(df$created))
return(df)
})
nTweets <- unlist(lapply(tweetFrames, function(df){
nrow(df)
}))
timeElapsed <- unlist(lapply(tweetFrames, function(df){
as.numeric(diff(range(df$timeStamp)), units = "secs")
}))
tweetsPerSec <- nTweets
plot(tweetsPerSec, type="h", frame.plot=FALSE,  xaxt="n")
axis(1, labels=names(tweetsPerSec), at=c(1:5))
axis(1, labels=names(tweetsPerSec))
axis(1, labels=names(tweetsPerSec), at=c(1:3))
CW <-(unclass(tweetFrames$Chemical Weapon$timeStamp)-1356100000)
CW <-(unclass(tweetFrames$ChemicalWeapon$timeStamp)-1356100000)
head(tweetFrames)
colname(tweetFrames)
ColName(tweetFrames)
ColNames(tweetFrames)
colnames(tweetFrames)
CW <-(unclass(tweetFrames$opcw$timeStamp)-1356100000)
CW <-(unclass(tweetFrames$timeStamp)-1356100000)
searchTerm <- "#opcw"
searchResults <- searchTwitter(searchTerm, n = 1000)
searchTerm <- "opcw"
searchResults <- searchTwitter(searchTerm, n = 1000)
tweetFrame <- twListToDF(searchResults)
View(tweetFrame)
userInfo <- lookupUsers(tweetFrame$screenName)
userFrame <- twListToDF(userInfo)
View(userFrame)
locatedUsers <- !is.na(userFrame$location)
locations <- geocode(userFrame$location[locatedUsers])
View(locations)
plot(locations$lon, locations$lat)
p1 <- ggplot(worldMap)
p2 <- p1 + geom_path(aes(x = long, y = lat, group = group),
colour = gray(2/3), lwd = 1/3)
p3 <- p2 + geom_point(data = locations,
aes(x = lon, y = lat),
colour = "RED", alpha = 1/2, size = 1)
print(p3)
library("ggmap")
library(maptools)
library(maps)
install.packages("ggmap")
library("ggmap")
library("geocode")
install.packages("geocodeHERE")
library("geocodeHERE")
library("ggmap")
library(maptools)
install.packages("maptools")
library(maptools)
library(dismo)
library(maps)
p1 <- ggplot(worldMap)
p2 <- p1 + geom_path(aes(x = long, y = lat, group = group),
colour = gray(2/3), lwd = 1/3)
p3 <- p2 + geom_point(data = locations,
aes(x = lon, y = lat),
colour = "RED", alpha = 1/2, size = 1)
print(p3)
consumerKey = "TnhFkCkWw5XiRjaaU6MVFJXrb"   # from your app name
consumerSecret = "8T2gfZ7hpCRAYcBQbkfURnyT3ylaHEa8BsO2akLz6gll6kN2OM"
accessToken = "377053028-LZYBzp2rcwn3sG103AVVdUvWHYrOBHLhNPP2wq5S"
accessSecret = "eFD67fq59GaltQj45I5F5eT8wEEdlCxEG3bRE3qod8ZVo"
setup_twitter_oauth(consumer_key = consumerKey, consumer_secret = consumerSecret,
access_token = accessToken, access_secret = accessSecret)
searchTerm <- "opcw"
searchResults <- searchTwitter(searchTerm, n = 1000)
tweetFrame <- twListToDF(searchResults)
library(stats)
library(base)
library(xml2)
library(rvest)
library(dplyr)
library(stats)
library(base)
library(xml2)
library(rvest)
library(dplyr)
library(sp)
library(raster)
library(twitteR)
library(lubridate)
library(RJSONIO)
library(ggplot2)
library(dismo)
library(maps)
library("geocodeHERE")
library("ggmap")
library(maptools)
a <- matrix(c(1,0,0,0,0,0,0,1/sqrt(2),1/sqrt(2),0,0,0,0,1/sqrt(2),-1/sqrt(2),0,0,0,0,0,0,1/sqrt(2),1/sqrt(2),0,0,0,0,1/sqrt(2),-1/sqrt(2),0,0,0,0,0,0,1), 6,6, byrow=T)
View(a)
b <- matrix(c(-1,0,0,0,0,0,0,0,1/2,0,0,0,0,0,1/2,0,0,0,0,0,0,1/4,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25), 6,6, byrow=T)
c <- matrix(c(1,0,0,0,0,0,0,1/sqrt(2),1/sqrt(2),0,0,0,0,1/sqrt(2),-1/sqrt(2),0,0,0,0,0,0,1/sqrt(2),1/sqrt(2),0,0,0,0,1/sqrt(2),-1/sqrt(2),0,0,0,0,0,0,1), 6,6, byrow=T)
z <- a*b*c
View(z)
View(a)
View(b)
b <- matrix(c(-1,0,0,0,0,0,0,1/2,0,0,0,0,0,0,1/2,0,0,0,0,0,0,1/4,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25), 6,6, byrow=T)
z <- a*b*c
View(z)
View(z)
dim(z)
eigen(z)
eigen(z^3)
file <- "<a href="https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv">https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv</a>"
library(RCurl)
library(bitops)
library(RCurl)
x <- getURL("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
csv_file <- getURL("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
y <- read.csv(text = csv_file)
csv_file_url <- getURL("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
csv_file <- read.csv(text = csv_file)
csv_file <- read.csv(text = csv_file_url)
df <- Filter(function(x)!all(is.na(x)), csv_file)
View(csv_file)
View(df)
csv_file <- read.csv(csv_file_url)
csv_file_url <- getURL("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
csv_file <- read.csv(csv_file_url)
csv_file_url <- getURL("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
csv_file <- read.csv(csv_file_url)
csv_file <- read.csv(url(csv_file_url))
csv_file <- import("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
library(devtools)
install_github("leeper/rio")
library(rio)
csv_file <- import("https://github.com/Amirosimani/ExploratoryDataAnalysis/blob/master/Survey.csv")
View(csv_file)
library(curl)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(SnowballC)
library(xlsx)
rm(list=ls())
#functions
clean.corpus <- function(txt){
text_corpus <- tm_map(text_corpus, stemDocument)
text_corpus <- tm_map(text_corpus, removeWords, c(stopwords("english"), stopwords("SMART")))
text_corpus <- tm_map(text_corpus, removePunctuation)
#text_corpus <- tm_map(text_corpus, tolower)
text_corpus <- tm_map(text_corpus, stripWhitespace)
text_corpus <- tm_map(text_corpus, PlainTextDocument)
assign('text_corpus',text_corpus,envir=.GlobalEnv)
}
word.frequency <- function (TDM){
temp <- inspect(TDM)
word.frequency <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
row.names(word.frequency) <- NULL
assign('word.frequency',word.frequency,envir=.GlobalEnv)
word.frequency <- word.frequency[order(-word.frequency$Freq),]
}
data.dir<-"GitHub/Persian-Gardens"
#read the text file
setwd(data.dir)
fileName = "Donald Wilber Report.txt"
conn <- file(fileName,open="r")
text <-readLines(conn)
close(conn)
#Combining words that should stay together
for (j in seq(text)) {
text[[j]] <- gsub("Allen W. Dulles", "Allen_W_Dulles", text[[j]])
text[[j]] <- gsub("British secret intelligence service ", "SIS", text[[j]])
text[[j]] <- gsub("British Foreign Office", "BFO", text[[j]])
text[[j]] <- gsub("Department of State", "DepOfState", text[[j]])
text[[j]] <- gsub("Director of CIA", "DoCIA", text[[j]])
text[[j]] <- gsub("Foreign Office", "Foreign_Office", text[[j]])
text[[j]] <- gsub("General Fazllolah Zahedi", "Gen_Zahedi", text[[j]])
text[[j]] <- gsub("Kermit Roosevelt", "Kermit_Roosevelt", text[[j]])
text[[j]] <- gsub("General Fazllolah Zahedi", "Gen_Zahedi", text[[j]])
text[[j]] <- gsub("Loy Wesley Henderson", "Loy_Henderson", text[[j]])
text[[j]] <- gsub("military secretariat", "military_secretariat", text[[j]])
text[[j]] <- gsub("Norman Schwarzkopf", "Norman_Schwarzkopf", text[[j]])
text[[j]] <- gsub("President of the United States", "President_US", text[[j]])
text[[j]] <- gsub("Princess Ashraf Pahlavi", "Ashraf_Pahlavi", text[[j]])
text[[j]] <- gsub("Roger Goiran", "Roger_Goiran", text[[j]])
text[[j]] <- gsub("Secretary of State", "Secretary_of_State", text[[j]])
text[[j]] <- gsub("Tehran military", "Tehran_military", text[[j]])
}
#creat a cropus
text_corpus <- Corpus(VectorSource(text), readerControl = list(language = "en"))
#clean up the corpus
clean.corpus(text_corpus)
#(TDM) which reflects the number of times each word in the corpus is found in each of the documents
TDM <- TermDocumentMatrix(text_corpus,control = list(removePunctuation = TRUE,stopwords = TRUE))
inspect(TDM)
word.frequency(TDM)
write.xlsx(word.frequency, "./WordFrequency.xlsx")
#Creat the wordckoud
wordcloud(word.frequency$ST, word.frequency$Freq,max.words=100, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
library(cluster)
d <- dist(t(TDM), method="euclidian")
fit <- hclust(d=d, method="ward")
fit <- hclust(d=d, method="ward.D2")
fit
plot(fit, hang=-1)
dtmss <- removeSparseTerms(TDM, 0.15)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward.D2")
fit
plot(fit, hang=-1)
dtmss <- removeSparseTerms(dtm, 0.1)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward.D2")
fit
plot(fit, hang=-1)
dtmss <- removeSparseTerms(dtm, 0.9)
dtmss <- removeSparseTerms(TDM, 0.9)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward.D2")
plot(fit, hang=-1)
freq <- sort(colSums(as.matrix(TDM)), decreasing=TRUE)
View(word.frequency)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(SnowballC)
library(xlsx)
library(cluster)
rm(list=ls())
#functions
clean.corpus <- function(txt){
text_corpus <- tm_map(text_corpus, stemDocument)
text_corpus <- tm_map(text_corpus, removeWords, c(stopwords("english"), stopwords("SMART")))
text_corpus <- tm_map(text_corpus, removePunctuation)
#text_corpus <- tm_map(text_corpus, tolower)
text_corpus <- tm_map(text_corpus, stripWhitespace)
text_corpus <- tm_map(text_corpus, PlainTextDocument)
assign('text_corpus',text_corpus,envir=.GlobalEnv)
}
word.frequency <- function (TDM){
temp <- inspect(TDM)
word.frequency <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
row.names(word.frequency) <- NULL
word.frequency <- word.frequency[order(-word.frequency$Freq),]
assign('word.frequency',word.frequency,envir=.GlobalEnv)
}
data.dir<-"GitHub/Persian-Gardens"
#read the text file
setwd(data.dir)
fileName = "Donald Wilber Report.txt"
conn <- file(fileName,open="r")
text <-readLines(conn)
text_corpus <- Corpus(VectorSource(text), readerControl = list(language = "en"))
#clean up the corpus
clean.corpus(text_corpus)
TDM <- TermDocumentMatrix(text_corpus,control = list(removePunctuation = TRUE,stopwords = TRUE))
inspect(TDM)
word.frequency(TDM)
View(word.frequency)
for (j in seq(text)) {
text[[j]] <- gsub("Allen W. Dulles", "Allen_W_Dulles", text[[j]])
text[[j]] <- gsub("British secret intelligence service ", "SIS", text[[j]])
text[[j]] <- gsub("British Foreign Office", "BFO", text[[j]])
text[[j]] <- gsub("Department of State", "DepOfState", text[[j]])
text[[j]] <- gsub("Director of CIA", "DoCIA", text[[j]])
text[[j]] <- gsub("Foreign Office", "Foreign_Office", text[[j]])
text[[j]] <- gsub("General Fazllolah Zahedi", "Gen_Zahedi", text[[j]])
text[[j]] <- gsub("Kermit Roosevelt", "Kermit_Roosevelt", text[[j]])
text[[j]] <- gsub("General Fazllolah Zahedi", "Gen_Zahedi", text[[j]])
text[[j]] <- gsub("Loy Wesley Henderson", "Loy_Henderson", text[[j]])
text[[j]] <- gsub("military secretariat", "military_secretariat", text[[j]])
text[[j]] <- gsub("Norman Schwarzkopf", "Norman_Schwarzkopf", text[[j]])
text[[j]] <- gsub("President of the United States", "President_US", text[[j]])
text[[j]] <- gsub("Princess Ashraf Pahlavi", "Ashraf_Pahlavi", text[[j]])
text[[j]] <- gsub("Roger Goiran", "Roger_Goiran", text[[j]])
text[[j]] <- gsub("Secretary of State", "Secretary_of_State", text[[j]])
text[[j]] <- gsub("Tehran military", "Tehran_military", text[[j]])
}
text_corpus <- Corpus(VectorSource(text), readerControl = list(language = "en"))
clean.corpus(text_corpus)
TDM <- TermDocumentMatrix(text_corpus,control = list(removePunctuation = TRUE,stopwords = TRUE))
inspect(TDM)
word.frequency(TDM)
word.frequency <- function (TDM){
temp <- inspect(TDM)
word.frequency <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
row.names(word.frequency) <- NULL
word.frequency <- word.frequency[order(-word.frequency$Freq),]
assign('word.frequency',word.frequency,envir=.GlobalEnv)
}
word.frequency(TDM)
View(word.frequency)
write.xlsx(word.frequency, "./WordFrequency.xlsx")
View(word.frequency)
word.fre.sub <- subset(word.frequency, ST > 15)
word.fre.sub <- subset(word.frequency, word.frequency$ST > 15)
word.fre.sub <- subset(word.frequency, as.numeric(word.frequency$ST) > 15)
View(word.fre.sub)
word.fre.sub <- subset(word.frequency, word.frequency$Freq) > 15)
word.fre.sub <- subset(word.frequency, as.numeric(word.frequency$Freq) > 15)
View(word.fre.sub)
head(TDM)
summary(TDM)
inspect(TDM)
dtm <- DocumentTermMatrix(text_corpus)
dtmss <- removeSparseTerms(dtm, 0.1)
inspect(dtmss)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward.D2")
plot(fit, hang=-1)
dtmss <- removeSparseTerms(TDM, 0.1)
inspect(dtmss)
dtmss <- removeSparseTerms(TDM, 0.9)
inspect(dtmss)
dtmss <- removeSparseTerms(TDM, 0.0001)
inspect(dtmss)
d <- dist(t(), method="euclidian")
fit <- hclust(d=d, method="ward.D2")
fit
plot(fit, hang=-1)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(SnowballC)
library(xlsx)
library(cluster)
rm(list=ls())
#functions
clean.corpus <- function(txt){
text_corpus <- tm_map(text_corpus, stemDocument)
text_corpus <- tm_map(text_corpus, removeWords, c(stopwords("english"), stopwords("SMART")))
text_corpus <- tm_map(text_corpus, removePunctuation)
#text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, stripWhitespace)
text_corpus <- tm_map(text_corpus, PlainTextDocument)
assign('text_corpus',text_corpus,envir=.GlobalEnv)
}
word.frequency <- function (TDM){
temp <- inspect(TDM)
word.frequency <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
row.names(word.frequency) <- NULL
word.frequency <- word.frequency[order(-word.frequency$Freq),]
assign('word.frequency',word.frequency,envir=.GlobalEnv)
}
data.dir<-"GitHub/Persian-Gardens"
#read the text file
setwd(data.dir)
fileName = "Donald Wilber Report.txt"
conn <- file(fileName,open="r")
text <-readLines(conn)
close(conn)
#Combining words that should stay together
for (j in seq(text)) {
text[[j]] <- gsub("Allen W. Dulles", "Allen_W_Dulles", text[[j]])
text[[j]] <- gsub("British secret intelligence service ", "SIS", text[[j]])
text[[j]] <- gsub("British Foreign Office", "BFO", text[[j]])
text[[j]] <- gsub("Department of State", "DepOfState", text[[j]])
text[[j]] <- gsub("Director of CIA", "DoCIA", text[[j]])
text[[j]] <- gsub("Foreign Office", "Foreign_Office", text[[j]])
text[[j]] <- gsub("General Fazllolah Zahedi", "Gen_Zahedi", text[[j]])
text[[j]] <- gsub("Kermit Roosevelt", "Kermit_Roosevelt", text[[j]])
text[[j]] <- gsub("General Fazllolah Zahedi", "Gen_Zahedi", text[[j]])
text[[j]] <- gsub("Loy Wesley Henderson", "Loy_Henderson", text[[j]])
text[[j]] <- gsub("military secretariat", "military_secretariat", text[[j]])
text[[j]] <- gsub("Norman Schwarzkopf", "Norman_Schwarzkopf", text[[j]])
text[[j]] <- gsub("President of the United States", "President_US", text[[j]])
text[[j]] <- gsub("Princess Ashraf Pahlavi", "Ashraf_Pahlavi", text[[j]])
text[[j]] <- gsub("Roger Goiran", "Roger_Goiran", text[[j]])
text[[j]] <- gsub("Secretary of State", "Secretary_of_State", text[[j]])
text[[j]] <- gsub("Tehran military", "Tehran_military", text[[j]])
}
#creat a cropus
text_corpus <- Corpus(VectorSource(text), readerControl = list(language = "en"))
#clean up the corpus
clean.corpus(text_corpus)
#(TDM) which reflects the number of times each word in the corpus is found in each of the documents
TDM <- TermDocumentMatrix(text_corpus,control = list(removePunctuation = TRUE,stopwords = TRUE))
inspect(TDM)
word.frequency(TDM)
write.xlsx(word.frequency, "./WordFrequency.xlsx")
#Creat the wordckoud
wordcloud(word.frequency$ST, word.frequency$Freq,max.words=100, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(word.frequency$ST, word.frequency$Freq,max.words=100, scale=c(5, .1))
wordcloud(word.frequency$ST, word.frequency$Freq,max.words=1000, scale=c(5, .1))
wordcloud(word.frequency$ST, word.frequency$Freq,max.words=2000, scale=c(5, .1))
